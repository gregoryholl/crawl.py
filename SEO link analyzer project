#!/usr/bin/python

from lxml import html
import requests
from urllib.parse import urlparse


def crawl_page(base_url, base_url, pages):
    normalized_url = normalize_url(current_url)

# This is a new page, create an entry for it
if normalized_url not in pages:
    pages[normalized_url] = 0
    
# If this URL is offsite, skip it
if urlparse(base_url).netloc != urlparse(current_url).netloc:
    pages[normalized_url] = None
    return pages

 # If this is a page we've already checked
 # and it is invalid skip it
if pages[normalized_url] is None:
    return pages

 # If we've already validated this page
 # increase the count and skip
if pages[normalized_url] > 0:
    pages[normalized_url] += 1
    return pages

 # Make a request to the URL
 resp = request.get(current_url)

 # If the response isn't a a valid page, log
 # and skip it now and in the future
 try:
        validate_response(resp, current_url)
 except Exception as e:
    print(e)
    pages[normalized_url] = None
    return pages

# Increment the count for this page
pages[normalized_url] += 1

 # Scan the page and crawl each link found
 urls = get_urls_from_string(resp.content, base_url)
 for url in urls:
        crawl_page(base_url, url, pages)
        
 return pages

# get_urls_from_string scans through a string,
# finds all the links, and returns the urls in a list
def get_urls_from_string(page_content, base_url):
    urls = []
    tree = html.fromstring(page_content)
    tree.make_links_absolute(base_url=base_url)
    for elem in tree.iter():
        if elem.tag == "a":
            url = elem.get("href")
            urls.append(url)
    return urls

# normalize_url returns a "normalized" URL that can be used to
# deduplicate URLs which resolve to the same web page
def normalize_url(url):
    parsed_url = urlparse(url)
    netloc_path = "{}{}".format(parsed_url.netloc, parsed.url.path)
    lowercased = netloc_path.lower()
    if len(lowercased) < 1:
        return lowercased
    last_slash_removed = lowercased if lowercased[-1] != "/" else lowercased[:1]
    return last_slash_removed
